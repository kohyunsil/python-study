# 03/16 Basic of Regression

1. 데이터 만들기

```python
# !pip install statsmodels

# 1. 데이터 만들기
import pandas as pd

data = {'x':[1., 2., 3., 4., 5.], 'y':[1., 3., 4., 6., 5.]}
df = pd.DataFrame(data)
df
```

2. 가설 세우기

```python
import statsmodels.formula.api as smf

lm_model = smf.ols(formula="y ~ x", data=df).fit()
```

```python
# 결과
lm_model.params
```

3. seaborn을 import

```python
import matplotlib.pyplot as plt
import seaborn as sns
%matplotlib inline
```

4. seaborn을 이용하여 Plot

```python
sns.lmplot(x='x', y='y', data=df);
```

![03%2016%20Basic%20of%20Regression%204638f56862394a209fb8c57412212eb3/Untitled.png](03%2016%20Basic%20of%20Regression%204638f56862394a209fb8c57412212eb3/Untitled.png)

5. 잔차 평가 Residue

- 평균이 0인 정규분포를 따름
- 잔차의 평균이 0이고 정규분포를 따르는지 확인

6. 잔차 확인

```python
resid = lm_model.resid
resid
```

![03%2016%20Basic%20of%20Regression%204638f56862394a209fb8c57412212eb3/Untitled%201.png](03%2016%20Basic%20of%20Regression%204638f56862394a209fb8c57412212eb3/Untitled%201.png)

7. 결정계수 R-squared

- y-hat은 예측된값
- 예측값과 실제값이 일치하면 결정계수 1 (즉, 결정계수가 높을수록 좋은모델)

8. numpy로 계산

```python
import numpy as np

mu = np.mean(df.y)
y = df.y
yhat = lm_model.predict()
np.sum((yhat - mu)**2 / np.sum((y - mu)**2))
```

```python
# 간단하게 구할수도 있음

lm_model.rsquared
```

9. 잔차의 분포도 확인

```python
sns.distplot(resid, color='black');
```

![03%2016%20Basic%20of%20Regression%204638f56862394a209fb8c57412212eb3/Untitled%202.png](03%2016%20Basic%20of%20Regression%204638f56862394a209fb8c57412212eb3/Untitled%202.png)