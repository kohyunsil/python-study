# ë³€í™”ìœ¨ê³¼ ë„í•¨ìˆ˜

ë³€í™”ìœ¨ì˜ ê²€ì¦ì„ ìœ„í•œ ì‚¬ì „ ìž‘ì—…

```python
import numpy as np

x = 5
```

```python
f = lambda x : x**2
df = lambda x : 2*x

x, f(x), df(x)
```

ë³€í™”ìœ¨ì˜ ê²€ì¦

```python
delta_x = 0.01
df(x), (f(x+delta_x)-f(x))/delta_x
```

```python
delta_x = 0.00001
df(x), (f(x+delta_x)-f(x))/delta_x
```

ìˆœê°„ë³€í™”ìœ¨

ðŸ¥‘ ë‹¤í•­í•¨ìˆ˜ì˜ ë¯¸ë¶„ì„ ìœ„í•œ í•¨ìˆ˜ ì„ ì •

```python
import sympy as sym

x = sym.Symbol('x')
y = x**2 + 2*x
y
```

ðŸ¥‘ ë‹¤í•­í•¨ìˆ˜ì˜ ë¯¸ë¶„

```python
sym.diff(y,x)
```

ðŸ¥‘ exponential í•¨ìˆ˜ì˜ ë¯¸ë¶„ì€ ìžê¸° ìžì‹ ì´ë‹¤

```python
y = sym.exp(x)
y

sym.diff(y,x
```

ðŸ¥‘  ìžì—°ë¡œê·¸

```python
y = sym.ln(x)
y

sym.diff(y, x)
```

í•©ì„±í•¨ìˆ˜

```python
y = x**2 + 3*x
h = y**2
h
```

ðŸ¥‘ í•©ì„±í•¨ìˆ˜ì˜ ë¯¸ë¶„

```python
sym.diff(h,x)
```

Logistic Function

```python
import numpy as np

z = np.arange(-10, 10, 0.01)
g = 1 / (1+np.exp(-z))
```

```python
import matplotlib.pyplot as plt
%matplotlib inline

plt.plot(z, g);
```

![%E1%84%87%E1%85%A7%E1%86%AB%E1%84%92%E1%85%AA%E1%84%8B%E1%85%B2%E1%86%AF%E1%84%80%E1%85%AA%20%E1%84%83%E1%85%A9%E1%84%92%E1%85%A1%E1%86%B7%E1%84%89%E1%85%AE%20be0c7978ab3540db9655a8fe78737ec1/Untitled.png](%E1%84%87%E1%85%A7%E1%86%AB%E1%84%92%E1%85%AA%E1%84%8B%E1%85%B2%E1%86%AF%E1%84%80%E1%85%AA%20%E1%84%83%E1%85%A9%E1%84%92%E1%85%A1%E1%86%B7%E1%84%89%E1%85%AE%20be0c7978ab3540db9655a8fe78737ec1/Untitled.png)

```python
plt.figure(figsize=(12,8))
ax = plt.gca()

ax.plot(z,g)
ax.spines['left'].set_position('zero')
ax.spines['right'].set_color('none')
ax.spines['bottom'].set_position('center')
ax.spines['top'].set_color('none')

plt.show()
```

![%E1%84%87%E1%85%A7%E1%86%AB%E1%84%92%E1%85%AA%E1%84%8B%E1%85%B2%E1%86%AF%E1%84%80%E1%85%AA%20%E1%84%83%E1%85%A9%E1%84%92%E1%85%A1%E1%86%B7%E1%84%89%E1%85%AE%20be0c7978ab3540db9655a8fe78737ec1/Untitled%201.png](%E1%84%87%E1%85%A7%E1%86%AB%E1%84%92%E1%85%AA%E1%84%8B%E1%85%B2%E1%86%AF%E1%84%80%E1%85%AA%20%E1%84%83%E1%85%A9%E1%84%92%E1%85%A1%E1%86%B7%E1%84%89%E1%85%AE%20be0c7978ab3540db9655a8fe78737ec1/Untitled%201.png)

hypothesis í•¨ìˆ˜ì˜ ê²°ê³¼ì— ë”°ë¥¸ ë¶„ë¥˜

```python

# Logistic Reg. Cost Fcnì˜ ê·¸ëž˜í”„

h = np.arange(0.01, 1, 0.01)

C0 = -np.log(1-h)
C1 = -np.log(h)

plt.figure(figsize=(12,8))
plt.plot(h, C0, label='y=0')
plt.plot(h, C1, label='y=1')
plt.legend()
plt.show()
```

![%E1%84%87%E1%85%A7%E1%86%AB%E1%84%92%E1%85%AA%E1%84%8B%E1%85%B2%E1%86%AF%E1%84%80%E1%85%AA%20%E1%84%83%E1%85%A9%E1%84%92%E1%85%A1%E1%86%B7%E1%84%89%E1%85%AE%20be0c7978ab3540db9655a8fe78737ec1/Untitled%202.png](%E1%84%87%E1%85%A7%E1%86%AB%E1%84%92%E1%85%AA%E1%84%8B%E1%85%B2%E1%86%AF%E1%84%80%E1%85%AA%20%E1%84%83%E1%85%A9%E1%84%92%E1%85%A1%E1%86%B7%E1%84%89%E1%85%AE%20be0c7978ab3540db9655a8fe78737ec1/Untitled%202.png)

ë°ì´í„°ë°›ê¸°

```python
import pandas as pd

wine_url = 'https://raw.githubusercontent.com/PinkWink/ML_tutorial/master/dataset/wine.csv'
wine = pd.read_csv(wine_url, index_col=0)
wine.head()
```

ë§› ë“±ê¸‰ ë§Œë“¤ì–´ ë„£ê¸°

```python
wine['taste'] = [1. if grade>5 else 0. for grade in wine['quality']]

X = wine.drop(['taste', 'quality'], axis=1)
y = wine['taste']

# ë°ì´í„° ë¶„ë¦¬
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=13)
```

ë¡œì§€ìŠ¤í‹± íšŒê·€ í…ŒìŠ¤íŠ¸

```python
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

lr = LogisticRegression(solver='liblinear', random_state=13)
lr.fit(X_train, y_train)

y_pred_tr = lr.predict(X_train)
y_pred_test = lr.predict(X_test)

print('Train Acc : ', accuracy_score(y_train, y_pred_tr))
print('Test Acc : ', accuracy_score(y_test, y_pred_test))
```

ìŠ¤ì¼€ì¼ëŸ¬ ì ìš©í•´ì„œ íŒŒì´í”„ë¼ì¸ êµ¬ì¶•

```python
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler

estimators = [('scaler', StandardScaler()),
             ('clf', LogisticRegression(solver='liblinear', random_state=13))]

pipe_lr = Pipeline(estimators)
```

fit

```python
pipe_lr.fit(X_train, y_train)
```

ìƒìŠ¹ íš¨ê³¼ê°€ ìžˆê¸´í•˜ë‹¤

```python
y_pred_tr = pipe.predict(X_train)
y_pred_test = pipe.predict(X_test)

print('Train Acc : ', accuracy_score(y_train, y_pred_tr))
print('Test Acc : ', accuracy_score(y_test, y_pred_test))
```

Decision Treeì™€ì˜ ë¹„êµë¥¼ ìœ„í•œ ìž‘ì—…

```python
from sklearn.tree import DecisionTreeClassifier

wine_tree = DecisionTreeClassifier(max_depth=2, random_state=13)
wine_tree.fit(X_train, y_train)

models = {'logistic regression':pipe_lr, 'decision tree':wine_tree}
```

AUC ê·¸ëž˜í”„ë¥¼ ì´ìš©í•œ ëª¨ë¸ê°„ ë¹„êµ

```python
from sklearn.metrics import roc_curve

plt.figure(figsize=(10,8))
plt.plot([0,1], [0,1])
for model_name, model in models.items():
    pred = model.predict_proba(X_test)[:, 1]
    fpr, tpr, thresholds = roc_curve(y_test, pred)
    plt.plot(fpr, tpr, label=model_name)

plt.grid()
plt.legend()
plt.show()
```

![%E1%84%87%E1%85%A7%E1%86%AB%E1%84%92%E1%85%AA%E1%84%8B%E1%85%B2%E1%86%AF%E1%84%80%E1%85%AA%20%E1%84%83%E1%85%A9%E1%84%92%E1%85%A1%E1%86%B7%E1%84%89%E1%85%AE%20be0c7978ab3540db9655a8fe78737ec1/Untitled%203.png](%E1%84%87%E1%85%A7%E1%86%AB%E1%84%92%E1%85%AA%E1%84%8B%E1%85%B2%E1%86%AF%E1%84%80%E1%85%AA%20%E1%84%83%E1%85%A9%E1%84%92%E1%85%A1%E1%86%B7%E1%84%89%E1%85%AE%20be0c7978ab3540db9655a8fe78737ec1/Untitled%203.png)

ì¸ë””ì–¸ ë‹¹ë‡¨ë³‘

```python
# ì¸ë””ì–¸ ë‹¹ë‡¨ë³‘

import pandas as pd

PIMA_url = 'https://raw.githubusercontent.com/PinkWink/ML_tutorial/master/dataset/diabetes.csv'

PIMA = pd.read_csv(PIMA_url)
PIMA.head()
```

```python
PIMA.info()
```

float ìœ¼ë¡œ ë°ì´í„° ë³€í™˜

```python
PIMA = PIMA.astype('float')
PIMA.info()
```

ì¼ë‹¨ ìƒê´€ê´€ê³„ í™•ì¸

```python
import seaborn as sns
import matplotlib.pyplot as plt
%matplotlib inline

plt.figure(figsize=(12,10))
sns.heatmap(PIMA.corr(), cmap="YlGnBu")
plt.show()
```

![%E1%84%87%E1%85%A7%E1%86%AB%E1%84%92%E1%85%AA%E1%84%8B%E1%85%B2%E1%86%AF%E1%84%80%E1%85%AA%20%E1%84%83%E1%85%A9%E1%84%92%E1%85%A1%E1%86%B7%E1%84%89%E1%85%AE%20be0c7978ab3540db9655a8fe78737ec1/Untitled%204.png](%E1%84%87%E1%85%A7%E1%86%AB%E1%84%92%E1%85%AA%E1%84%8B%E1%85%B2%E1%86%AF%E1%84%80%E1%85%AA%20%E1%84%83%E1%85%A9%E1%84%92%E1%85%A1%E1%86%B7%E1%84%89%E1%85%AE%20be0c7978ab3540db9655a8fe78737ec1/Untitled%204.png)

ê·¸ëŸ°ë° 0ì´ ìžˆë‹¤, PIMA ì¸ë””ì–¸ì— ëŒ€í•œ ì •ë³´ê°€ ì—†ìœ¼ë¯€ë¡œ í‰ê· ê°’ìœ¼ë¡œ ëŒ€ì²´

```python
(PIMA==0).astype(int).sum()

zero_features = ['Glucose', 'BloodPressure', 'SkinThickness', 'BMI']
PIMA[zero_features] = PIMA[zero_features].replace(0, PIMA[zero_features].mean())
(PIMA==0).astype(int).sum()
```

ë°ì´í„°ë¥¼ ë‚˜ëˆ„ê³ 

```python
from sklearn.model_selection import train_test_split

X = PIMA.drop(['Outcome'], axis=1)
y = PIMA['Outcome']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,
                                                   random_state = 13,
                                                   stratify = y)
```

Pipelineì„ ë§Œë“¤ê¸°

```python
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression

estimators = [('scaler', StandardScaler()),
             ('clf', LogisticRegression(solver='liblinear', random_state=13))]

pipe_lr = Pipeline(estimators)
pipe_lr.fit(X_train, y_train)
pred = pipe_lr.predict(X_test)
```

```python
from sklearn.metrics import (accuracy_score, recall_score, precision_score,
                            roc_auc_score, f1_score)

print('Accuracy : ', accuracy_score(y_test, pred))
print('Recall : ', recall_score(y_test, pred))
print('Precision : ', precision_score(y_test, pred))
print('AUC score : ', roc_auc_score(y_test, pred))
print('f1 score : ', f1_score(y_test, pred))
```

![%E1%84%87%E1%85%A7%E1%86%AB%E1%84%92%E1%85%AA%E1%84%8B%E1%85%B2%E1%86%AF%E1%84%80%E1%85%AA%20%E1%84%83%E1%85%A9%E1%84%92%E1%85%A1%E1%86%B7%E1%84%89%E1%85%AE%20be0c7978ab3540db9655a8fe78737ec1/Untitled%205.png](%E1%84%87%E1%85%A7%E1%86%AB%E1%84%92%E1%85%AA%E1%84%8B%E1%85%B2%E1%86%AF%E1%84%80%E1%85%AA%20%E1%84%83%E1%85%A9%E1%84%92%E1%85%A1%E1%86%B7%E1%84%89%E1%85%AE%20be0c7978ab3540db9655a8fe78737ec1/Untitled%205.png)

ë‹¤ë³€ìˆ˜ ë°©ì •ì‹ì˜ ê° ê³„ìˆ˜ê°’ì„ í™•ì¸

```python
#ê³„ìˆ˜ê°’
coeff = list(pipe_lr['clf'].coef_[0])
labels = list(X_train.columns)

coeff
```

![%E1%84%87%E1%85%A7%E1%86%AB%E1%84%92%E1%85%AA%E1%84%8B%E1%85%B2%E1%86%AF%E1%84%80%E1%85%AA%20%E1%84%83%E1%85%A9%E1%84%92%E1%85%A1%E1%86%B7%E1%84%89%E1%85%AE%20be0c7978ab3540db9655a8fe78737ec1/Untitled%206.png](%E1%84%87%E1%85%A7%E1%86%AB%E1%84%92%E1%85%AA%E1%84%8B%E1%85%B2%E1%86%AF%E1%84%80%E1%85%AA%20%E1%84%83%E1%85%A9%E1%84%92%E1%85%A1%E1%86%B7%E1%84%89%E1%85%AE%20be0c7978ab3540db9655a8fe78737ec1/Untitled%206.png)

```python
features = pd.DataFrame({'Features':labels, 'importance':coeff})
features.sort_values(by=['importance'], ascending=True, inplace=True)
features['positive']= features['importance'] > 0
features.set_index('Features', inplace=True)
features['importance'].plot(kind='barh',
                            figsize=(11, 6),
                            color=features['positive'].map({True:'blue', False:'red'}))

plt.xlabel('Importance')
plt.show()
```

![%E1%84%87%E1%85%A7%E1%86%AB%E1%84%92%E1%85%AA%E1%84%8B%E1%85%B2%E1%86%AF%E1%84%80%E1%85%AA%20%E1%84%83%E1%85%A9%E1%84%92%E1%85%A1%E1%86%B7%E1%84%89%E1%85%AE%20be0c7978ab3540db9655a8fe78737ec1/Untitled%207.png](%E1%84%87%E1%85%A7%E1%86%AB%E1%84%92%E1%85%AA%E1%84%8B%E1%85%B2%E1%86%AF%E1%84%80%E1%85%AA%20%E1%84%83%E1%85%A9%E1%84%92%E1%85%A1%E1%86%B7%E1%84%89%E1%85%AE%20be0c7978ab3540db9655a8fe78737ec1/Untitled%207.png)